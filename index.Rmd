---
title       : Hypothesis Testing & Regression
subtitle    : 
author      : Kris Sankaran
job         : 
framework: revealjs
revealjs: {theme: solarized}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---

## Introduction to Hypothesis Testing and Linear Regression

---

# Statistical Inference

*Goal*: Test scientific claims and quantify our uncertainty about it.

+ Comparing means between groups: the $t$-test.
    - $p$-values
    - Confidence intervals
+ Associating quantitative variables.
    - Linear regression

All the code for this presentation is available [here](https://github.com/krisrs1128/intro-to-testing)

---
Comparing means
-

- Is the mean of one sample significantly larger than another?
- Apply the hypothesis testing framework.
   + Initially assume there is no difference in means (*Null* hypothesis).
   + Require proof that the difference is nonzero (*Alternative* hypothesis).

```{r comparing_means, echo = F}
n.points <- 50
X.true.diff <- data.frame(x = c(rnorm(n.points), rnorm(n.points, .5)), group = factor(c(rep("A", n.points), rep("B", n.points))))
library("ggplot2")
X.no.diff <- data.frame(x = rnorm(2 * n.points, 0, 1), group = factor(c(rep("A", n.points), rep("B", n.points))))
```

```{r}
head(X.true.diff)

```

---

Simulated data with true difference in means:
```{r comparing_means_plot_2, echo = F, fig.align = "center"}
ggplot(X.true.diff) + geom_boxplot(aes(x = group, y = x)) + ggtitle("True difference")
```

---

Simulated data with no difference in means:

```{r comparing_fake_means_2, echo = F, fig.align = "center"}
ggplot(X.no.diff) + geom_boxplot(aes(x = group, y = x)) + ggtitle("No true difference")
```

--- 
## $t$-test

To quantitatively assess the difference in means, calculate
$$t = \frac{\sqrt{n}\left(\bar{X}_{A} - \bar{X}_{B}\right)}{\sqrt{\widehat{Var{X}}}}.$$

- $n$ is the total number of data points.
- $\bar{X}_{groups}$ is the mean in that group
-  $\widehat{Var{X}}$ is the sample variance, a measure of the "spread" of that group.

---

If there is no difference between the groups, we know the 
distribution of $t$, as long as a few extra assumptions hold, 
- The variances between the two groups are similar.
- Different samples are independent of each other.
- Either data are close to normal (bell-shaped), or there are many (> 30, say) samples.

```{r t-dist, fig.height = 5, echo = F}
x.axis <- seq(-4, 4, length.out = 1000)
qplot(x.axis, dt(x.axis, df = 2 * n.points), geom = "line") + 
ggtitle("t-statistic frequency curve, when no difference") +
scale_x_continuous("t") + 
scale_y_continuous("probability")
```

---
## Application to previous data

```{r t_testing}
# True difference
t.test(x ~ group, data = X.true.diff)
```

---

```{r t_testing_fake}
# No true difference
t.test(x ~ group, data = X.no.diff)
```

--- 
## Interpretation

+ $p$-value: The probability of a false positive (area under the frequency curve that is more extreme than the observed statistic).
+ Confidence interval: If we repeat the experiment, this (random) interval 
  will contain the true difference in means with 95% probability.
+ Both of these quantities are more informative than simply reporting whether we accept or reject the null hypothesis.

--- 

Visualizing observed $t$-statistics:

```{r t_with_cutoffs, echo = F}
qplot(x.axis, dt(x.axis, df = 2 * n.points), geom = "line") + 
ggtitle("t-statistic frequency curve, when no difference") +
scale_x_continuous("t") + 
scale_y_continuous("probability") + 
geom_vline(x = t.test(x ~ group, data = X.no.diff)$statistic, col = "steelblue") + 
geom_vline(x = t.test(x ~ group, data = X.true.diff)$statistic, col = "orange")
```

---


Real world example
-

Data from the [EMI music hackathon](https://www.kaggle.com/c/MusicHackathon).

```{r music_test, echo = F, warning = F, fig.align = "center"}
users <- read.csv("users.csv", row.names = 1)
users.impt <- subset(users, music %in% c("very.impt", "no.longer.impt"))
ggplot(users.impt) + geom_boxplot(aes(x = music, y = age)) + ggtitle("Music importance vs. Age")
```

---

```{r music_test_output}
t.test(age ~ music, data = users.impt)
```

---

Linear Regression
-

- Can we estimate the relationship between variables? (red is truth, blue is an estimate)
- Can we quantify our uncertainty about the estimate, 
  when we don't actually know the truth?

```{r lm-example, echo = F, fig.align = "center", fig.height = 5}
x <- rnorm(2 * n.points)
XY <- data.frame(x = x, y = 1.3 * x + .8 + rnorm(2 * n.points, 0, 2))
XY.model <- lm(y ~ x, data = XY)
ggplot(XY) + geom_point(aes(x = x, y = y)) + 
   geom_abline(aes(slope = 1.3, intercept = .8), col = "steelblue", size = 2) + 
   geom_abline(intercept = coefficients(XY.model)[1], slope = coefficients(XY.model)[2], col = "orange", size = 2)
```

---

## Application to previous data

- The interpretation of $p$-values and confidence intervals remains the same.
- We also have estimates of the slope and intercept for the above line.

```{r}
XY.model <- lm(y ~ x, data = XY)
summary(XY.model)

```

---
```{r} 
confint(XY.model)
```

---

Real world application
-

Users answered, on a scale of 0 to 100,
whether "I like to be at the cutting edge of 
new music" and "I like to know about music before
other people" (they also asked other questions).
```{r echo = F, warning = F, fig.align = "center"}
questions.model <- lm(Q18 ~ Q19, data  = users)
ggplot(users, aes(x = Q18, y = Q19)) +
    geom_point(alpha = 0.2) + 
    geom_abline(intercept = coefficients(questions.model)[1],
    		slope = coefficients(questions.model)[2], col = "steelblue", 
		size = 2)
```

--- 

```{r questions, warning = F}
summary(lm(Q19 ~ Q18, data  = users))
```

---

The estimated regression line seems reasonable, but the $p$-values
should not be trusted, because the *independence* and *linearity* assumptions
seem violated. In this case, the regression model is useful for prediction, 
but not for testing.

---

Here are some other useful statistical ideas

- Testing difference in proportions
- Testing in categorical data (the $\chi^{2}$-test)
- Model assessment and diagnostics
- Transforming features for regression
- Confounding, and how to deal with it
- Doing inference when linear models fail: using the "bootstrap"
